{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.util import ngrams\n",
    "import os\n",
    "from Model_List import model_paths\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "import torch\n",
    "\n",
    "# Downloading the stopwords and wordnet resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\rohan\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token = os.getenv('HUGGING_FACE'), add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52472, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./descriptions.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1996,  4208,  4874,  1999,  1996,  3746,  2003,  1037,  2235,\n",
      "          1010,  2317, 13297,  2007,  1037,  2630,  1998,  2317, 13478,  1012,\n",
      "          2009,  2038,  2048,  5209,  2006,  2169,  3358,  1010,  1998,  2045,\n",
      "          2024,  3645,  2006,  1996,  3903,  1012,  1996, 13297,  3544,  2000,\n",
      "          2022, 17337,  1010,  2007,  2053,  2060,  5200,  2030,  4506,  2635,\n",
      "          2173,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohan\\anaconda3\\envs\\three\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example text\n",
    "sample_text = \"The focused object in the image is a small, white airplane with a blue and white fuselage. It has two engines on each wing, and there are windows on the sides. The airplane appears to be stationary, with no other objects or actions taking place.\"\n",
    "\n",
    "# Tokenize using BERT tokenizer\n",
    "encoded_input = tokenizer(sample_text, return_tensors='pt')\n",
    "\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"openai-community/gpt2-medium\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# def summarize_text_with_gpt2(text):\n",
    "#     # Combine the instructional prompt with the text\n",
    "#     prompt = \"Briefly describe the text and don't mention about image, background, or any actions:\\n\" + text\n",
    "    \n",
    "#     # Encode the prompt to get input IDs\n",
    "#     inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    \n",
    "#     # Generate the output using the model\n",
    "#     outputs = model.generate(inputs, max_length=150, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "    \n",
    "#     # Decode and return the generated text\n",
    "#     summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return summary\n",
    "# long_text = 'The focussed object in the image is a small, white airplane with a blue and white fuselage. It has two engines on each wing, and there are windows on the sides. The airplane appears to be stationary, with no other objects or actions taking place.'\n",
    "# summary = summarize_text_with_gpt2(long_text)\n",
    "# print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db17f62bc336460e86ab2959b69eca2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model.to('cuda')\n",
    "# def get_summary(text):\n",
    "#     prompt_text = \"Summarize the following text into no more than three sentences: \" + text\n",
    "#     inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "#     outputs = model.generate(inputs.input_ids, max_length=150, num_return_sequences=1)\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# # Example usage\n",
    "# long_text = \"Here you insert the long, less meaningful text that you need summarized. This text might include multiple aspects such as historical data, technical details, and anecdotal evidence, but the summary should distill only the most essential information.\"\n",
    "# summary = get_summary(long_text)\n",
    "# print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_summary(text):\n",
    "#     prompt_text = \"Summarize the following text into no more than three sentences: \" + text\n",
    "#     inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "#     outputs = model.generate(inputs.input_ids, max_length=150, num_return_sequences=1)\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# # Example usage\n",
    "# long_text = \"The focussed object in the image is a small, white airplane with a blue and white fuselage. It has two engines on each wing, and there are windows on the sides. The airplane appears to be stationary, with no other objects or actions taking place.\"\n",
    "# summary = get_summary(long_text)\n",
    "# print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"openai-community/gpt2-medium\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# def summarize_text_with_gpt2(text):\n",
    "#     # Combine the instructional prompt with the text\n",
    "#     prompt = get_prompt(text)\n",
    "#     # Encode the prompt to get input IDs\n",
    "#     inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "#     # Generate the output using the model\n",
    "#     outputs = model.generate(inputs, max_length=500, num_return_sequences=1, no_repeat_ngram_size=2, pad_token_id=tokenizer.pad_token_id)\n",
    "#     outputs = model.generate(\n",
    "#     inputs,\n",
    "#     max_length=500,\n",
    "#     num_return_sequences=1,\n",
    "#     num_beams=1,           # Use a single beam for determinism\n",
    "#     no_repeat_ngram_size=2,\n",
    "#     do_sample=False,        # Turn off sampling; use deterministic decoding\n",
    "#     pad_token_id=tokenizer.pad_token_id\n",
    "# )\n",
    "#     # outputs = model.generate(inputs, max_length=500, num_return_sequences=1, pad_token_id=tokenizer.pad_token_id)\n",
    "#     # Decode and return the generated text\n",
    "#     summary = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "#     # summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return summary\n",
    "# long_text = 'The focussed object in the image is a small, white airplane with a blue and white fuselage. It has two engines on each wing, and there are windows on the sides. The airplane appears to be stationary, with no other objects or actions taking place.'\n",
    "# summary = summarize_text_with_gpt2(long_text)\n",
    "# print(summary.split('\\n')[-1])\n",
    "# model_name = 'facebook/bart-large-cnn'\n",
    "# tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "# model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "# def summarize_text(text):\n",
    "#     # Encode the text input and get the generated summary\n",
    "#     inputs = tokenizer.encode(\"summarize in one line\" + text, return_tensors='pt', max_length=1024, truncation=True)\n",
    "#     # summary_ids = model.generate(inputs, max_length=5000, min_length=4000, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "#     summary_ids = model.generate(inputs, max_length=500, num_beams=4, early_stopping=True)\n",
    "#     summary = tokenizer.decode(summary_ids[-1], skip_special_tokens=True)\n",
    "\n",
    "#     return summary\n",
    "\n",
    "# # Your text paragraph\n",
    "# text_paragraph = \"\"\"\n",
    "# The focussed object in the image is a small, white airplane with a blue and white fuselage. It has two engines on each wing, and there are windows on the sides. The airplane appears to be stationary, with no other objects or actions taking place.\n",
    "# \"\"\"\n",
    "\n",
    "# # Get the summary\n",
    "# summary = summarize_text(text_paragraph)\n",
    "# print(summary)\n",
    "# from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# # Initialize the tokenizer and model\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# # Few-shot examples followed by the new input\n",
    "# prompt = \"\"\"\n",
    "# Summarize the sentence \"I am going shopping because I don't have food in my fridge.\"\n",
    "# Summary:\n",
    "# \"\"\"\n",
    "\n",
    "# # Tokenize the prompt\n",
    "# inputs = tokenizer([prompt], max_length=1024, return_tensors='pt', truncation=True)\n",
    "\n",
    "# # Generate summary\n",
    "# summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
    "# summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# print(summary)\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# # Initialize the tokenizer and model\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "# model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# prompt = \"\"\"summarize: The focussed object in the image is a small, white airplane with a blue and white fuselage. It has two engines on each wing, and there are windows on the sides. The airplane appears to be stationary, with no other objects or actions taking place.\"\"\"\n",
    "# # Tokenize the prompt, setting max_length appropriately\n",
    "# inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "# # Generate summary\n",
    "# summary_ids = model.generate(inputs['input_ids'], num_beams=7, min_length=20, max_length=40)\n",
    "# summary = tokenizer.decode(summary_ids[0], skip_special_tokens=False)\n",
    "\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D visualization of text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the embeddings\n",
    "# datafile_path = \"./embedding.csv\"\n",
    "# df = pd.read_csv(datafile_path)\n",
    "# subclasses = df['Subclass']\n",
    "# # Convert to a list of lists of floats\n",
    "# matrix = np.array(df['Embedding'].apply(literal_eval).to_list())\n",
    "\n",
    "# # Create a t-SNE model and transform the data\n",
    "# tsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200)\n",
    "# vis_dims = tsne.fit_transform(matrix)\n",
    "# unique_subclasses = np.unique(subclasses)\n",
    "# subclass_map = {label: idx for idx, label in enumerate(unique_subclasses)}\n",
    "# color_labels = subclasses.map(subclass_map)  # Convert labels to integers\n",
    "# # Normalize your labels to be between 0 and 1\n",
    "# color_labels = (color_labels - color_labels.min()) / (color_labels.max() - color_labels.min())\n",
    "\n",
    "# cmap = cm.viridis\n",
    "# # Plot the results\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# scatter = plt.scatter(vis_dims[:, 0], vis_dims[:, 1], alpha=0.5, cmap=cm.viridis, c=color_labels)\n",
    "# plt.colorbar(scatter)  # Show color scale\n",
    "# plt.title('2D visualization of Text Embeddings')\n",
    "# plt.xlabel('Dimension 1')\n",
    "# plt.ylabel('Dimension 2')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D Visualization of text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from ast import literal_eval\n",
    "# from sklearn.manifold import TSNE\n",
    "# from matplotlib import pyplot as plt\n",
    "# from matplotlib import cm\n",
    "# from mpl_toolkits.mplot3d import Axes3D  # This is needed for 3D plotting\n",
    "\n",
    "# # Load the embeddings\n",
    "# datafile_path = \"./embedding.csv\"\n",
    "# df = pd.read_csv(datafile_path)\n",
    "# subclasses = df['Subclass']\n",
    "# # Convert to a list of lists of floats\n",
    "# matrix = np.array(df['Embedding'].apply(literal_eval).to_list())\n",
    "\n",
    "# # Create a t-SNE model and transform the data\n",
    "# tsne = TSNE(n_components=3, perplexity=15, random_state=42, init='random', learning_rate=200)\n",
    "# vis_dims = tsne.fit_transform(matrix)\n",
    "\n",
    "# # Retrieve unique subclasses and create a mapping\n",
    "# unique_subclasses = np.unique(subclasses)\n",
    "# subclass_map = {label: idx for idx, label in enumerate(unique_subclasses)}\n",
    "# color_labels = subclasses.map(subclass_map)  # Convert labels to integers\n",
    "\n",
    "# # Normalize your labels to be between 0 and 1\n",
    "# color_labels = (color_labels - color_labels.min()) / (color_labels.max() - color_labels.min())\n",
    "\n",
    "# cmap = cm.viridis\n",
    "\n",
    "# # Plot the results in 3D\n",
    "# fig = plt.figure(figsize=(12, 10))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# scatter = ax.scatter(vis_dims[:, 0], vis_dims[:, 1], vis_dims[:, 2], alpha=0.5, cmap=cm.viridis, c=color_labels)\n",
    "# plt.colorbar(scatter)  # Show color scale\n",
    "# ax.set_title('3D Visualization of Text Embeddings')\n",
    "# ax.set_xlabel('Dimension 1')\n",
    "# ax.set_ylabel('Dimension 2')\n",
    "# ax.set_zlabel('Dimension 3')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "three",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
