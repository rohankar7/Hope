{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.util import ngrams\n",
    "import os\n",
    "from Model_List import model_paths\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "import torch\n",
    "\n",
    "# Downloading the stopwords and wordnet resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\rohan\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token = os.getenv('HUGGING_FACE'), add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52472, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./descriptions.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1996,  4208,  4874,  1999,  1996,  3746,  2003,  1037,  2235,\n",
      "          1010,  2317, 13297,  2007,  1037,  2630,  1998,  2317, 13478,  1012,\n",
      "          2009,  2038,  2048,  5209,  2006,  2169,  3358,  1010,  1998,  2045,\n",
      "          2024,  3645,  2006,  1996,  3903,  1012,  1996, 13297,  3544,  2000,\n",
      "          2022, 17337,  1010,  2007,  2053,  2060,  5200,  2030,  4506,  2635,\n",
      "          2173,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohan\\anaconda3\\envs\\three\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example text\n",
    "sample_text = \"The focused object in the image is a small, white airplane with a blue and white fuselage. It has two engines on each wing, and there are windows on the sides. The airplane appears to be stationary, with no other objects or actions taking place.\"\n",
    "\n",
    "# Tokenize using BERT tokenizer\n",
    "encoded_input = tokenizer(sample_text, return_tensors='pt')\n",
    "\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"openai-community/gpt2-medium\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# def summarize_text_with_gpt2(text):\n",
    "#     # Combine the instructional prompt with the text\n",
    "#     prompt = \"Briefly describe the text and don't mention about image, background, or any actions:\\n\" + text\n",
    "    \n",
    "#     # Encode the prompt to get input IDs\n",
    "#     inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    \n",
    "#     # Generate the output using the model\n",
    "#     outputs = model.generate(inputs, max_length=150, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "    \n",
    "#     # Decode and return the generated text\n",
    "#     summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return summary\n",
    "# long_text = 'The focussed object in the image is a small, white airplane with a blue and white fuselage. It has two engines on each wing, and there are windows on the sides. The airplane appears to be stationary, with no other objects or actions taking place.'\n",
    "# summary = summarize_text_with_gpt2(long_text)\n",
    "# print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db17f62bc336460e86ab2959b69eca2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model.to('cuda')\n",
    "# def get_summary(text):\n",
    "#     prompt_text = \"Summarize the following text into no more than three sentences: \" + text\n",
    "#     inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "#     outputs = model.generate(inputs.input_ids, max_length=150, num_return_sequences=1)\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# # Example usage\n",
    "# long_text = \"Here you insert the long, less meaningful text that you need summarized. This text might include multiple aspects such as historical data, technical details, and anecdotal evidence, but the summary should distill only the most essential information.\"\n",
    "# summary = get_summary(long_text)\n",
    "# print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_summary(text):\n",
    "#     prompt_text = \"Summarize the following text into no more than three sentences: \" + text\n",
    "#     inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "#     outputs = model.generate(inputs.input_ids, max_length=150, num_return_sequences=1)\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# # Example usage\n",
    "# long_text = \"The focussed object in the image is a small, white airplane with a blue and white fuselage. It has two engines on each wing, and there are windows on the sides. The airplane appears to be stationary, with no other objects or actions taking place.\"\n",
    "# summary = get_summary(long_text)\n",
    "# print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"openai-community/gpt2-medium\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# def summarize_text_with_gpt2(text):\n",
    "#     # Combine the instructional prompt with the text\n",
    "#     prompt = get_prompt(text)\n",
    "#     # Encode the prompt to get input IDs\n",
    "#     inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "#     # Generate the output using the model\n",
    "#     outputs = model.generate(inputs, max_length=500, num_return_sequences=1, no_repeat_ngram_size=2, pad_token_id=tokenizer.pad_token_id)\n",
    "#     outputs = model.generate(\n",
    "#     inputs,\n",
    "#     max_length=500,\n",
    "#     num_return_sequences=1,\n",
    "#     num_beams=1,           # Use a single beam for determinism\n",
    "#     no_repeat_ngram_size=2,\n",
    "#     do_sample=False,        # Turn off sampling; use deterministic decoding\n",
    "#     pad_token_id=tokenizer.pad_token_id\n",
    "# )\n",
    "#     # outputs = model.generate(inputs, max_length=500, num_return_sequences=1, pad_token_id=tokenizer.pad_token_id)\n",
    "#     # Decode and return the generated text\n",
    "#     summary = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "#     # summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return summary\n",
    "# long_text = 'The focussed object in the image is a small, white airplane with a blue and white fuselage. It has two engines on each wing, and there are windows on the sides. The airplane appears to be stationary, with no other objects or actions taking place.'\n",
    "# summary = summarize_text_with_gpt2(long_text)\n",
    "# print(summary.split('\\n')[-1])\n",
    "# model_name = 'facebook/bart-large-cnn'\n",
    "# tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "# model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "# def summarize_text(text):\n",
    "#     # Encode the text input and get the generated summary\n",
    "#     inputs = tokenizer.encode(\"summarize in one line\" + text, return_tensors='pt', max_length=1024, truncation=True)\n",
    "#     # summary_ids = model.generate(inputs, max_length=5000, min_length=4000, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "#     summary_ids = model.generate(inputs, max_length=500, num_beams=4, early_stopping=True)\n",
    "#     summary = tokenizer.decode(summary_ids[-1], skip_special_tokens=True)\n",
    "\n",
    "#     return summary\n",
    "\n",
    "# # Your text paragraph\n",
    "# text_paragraph = \"\"\"\n",
    "# The focussed object in the image is a small, white airplane with a blue and white fuselage. It has two engines on each wing, and there are windows on the sides. The airplane appears to be stationary, with no other objects or actions taking place.\n",
    "# \"\"\"\n",
    "\n",
    "# # Get the summary\n",
    "# summary = summarize_text(text_paragraph)\n",
    "# print(summary)\n",
    "# from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# # Initialize the tokenizer and model\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# # Few-shot examples followed by the new input\n",
    "# prompt = \"\"\"\n",
    "# Summarize the sentence \"I am going shopping because I don't have food in my fridge.\"\n",
    "# Summary:\n",
    "# \"\"\"\n",
    "\n",
    "# # Tokenize the prompt\n",
    "# inputs = tokenizer([prompt], max_length=1024, return_tensors='pt', truncation=True)\n",
    "\n",
    "# # Generate summary\n",
    "# summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
    "# summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# print(summary)\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# # Initialize the tokenizer and model\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "# model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# prompt = \"\"\"summarize: The focussed object in the image is a small, white airplane with a blue and white fuselage. It has two engines on each wing, and there are windows on the sides. The airplane appears to be stationary, with no other objects or actions taking place.\"\"\"\n",
    "# # Tokenize the prompt, setting max_length appropriately\n",
    "# inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "# # Generate summary\n",
    "# summary_ids = model.generate(inputs['input_ids'], num_beams=7, min_length=20, max_length=40)\n",
    "# summary = tokenizer.decode(summary_ids[0], skip_special_tokens=False)\n",
    "\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D visualization of text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the embeddings\n",
    "# datafile_path = \"./embedding.csv\"\n",
    "# df = pd.read_csv(datafile_path)\n",
    "# subclasses = df['Subclass']\n",
    "# # Convert to a list of lists of floats\n",
    "# matrix = np.array(df['Embedding'].apply(literal_eval).to_list())\n",
    "\n",
    "# # Create a t-SNE model and transform the data\n",
    "# tsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200)\n",
    "# vis_dims = tsne.fit_transform(matrix)\n",
    "# unique_subclasses = np.unique(subclasses)\n",
    "# subclass_map = {label: idx for idx, label in enumerate(unique_subclasses)}\n",
    "# color_labels = subclasses.map(subclass_map)  # Convert labels to integers\n",
    "# # Normalize your labels to be between 0 and 1\n",
    "# color_labels = (color_labels - color_labels.min()) / (color_labels.max() - color_labels.min())\n",
    "\n",
    "# cmap = cm.viridis\n",
    "# # Plot the results\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# scatter = plt.scatter(vis_dims[:, 0], vis_dims[:, 1], alpha=0.5, cmap=cm.viridis, c=color_labels)\n",
    "# plt.colorbar(scatter)  # Show color scale\n",
    "# plt.title('2D visualization of Text Embeddings')\n",
    "# plt.xlabel('Dimension 1')\n",
    "# plt.ylabel('Dimension 2')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D Visualization of text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from ast import literal_eval\n",
    "# from sklearn.manifold import TSNE\n",
    "# from matplotlib import pyplot as plt\n",
    "# from matplotlib import cm\n",
    "# from mpl_toolkits.mplot3d import Axes3D  # This is needed for 3D plotting\n",
    "\n",
    "# # Load the embeddings\n",
    "# datafile_path = \"./embedding.csv\"\n",
    "# df = pd.read_csv(datafile_path)\n",
    "# subclasses = df['Subclass']\n",
    "# # Convert to a list of lists of floats\n",
    "# matrix = np.array(df['Embedding'].apply(literal_eval).to_list())\n",
    "\n",
    "# # Create a t-SNE model and transform the data\n",
    "# tsne = TSNE(n_components=3, perplexity=15, random_state=42, init='random', learning_rate=200)\n",
    "# vis_dims = tsne.fit_transform(matrix)\n",
    "\n",
    "# # Retrieve unique subclasses and create a mapping\n",
    "# unique_subclasses = np.unique(subclasses)\n",
    "# subclass_map = {label: idx for idx, label in enumerate(unique_subclasses)}\n",
    "# color_labels = subclasses.map(subclass_map)  # Convert labels to integers\n",
    "\n",
    "# # Normalize your labels to be between 0 and 1\n",
    "# color_labels = (color_labels - color_labels.min()) / (color_labels.max() - color_labels.min())\n",
    "\n",
    "# cmap = cm.viridis\n",
    "\n",
    "# # Plot the results in 3D\n",
    "# fig = plt.figure(figsize=(12, 10))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# scatter = ax.scatter(vis_dims[:, 0], vis_dims[:, 1], vis_dims[:, 2], alpha=0.5, cmap=cm.viridis, c=color_labels)\n",
    "# plt.colorbar(scatter)  # Show color scale\n",
    "# ax.set_title('3D Visualization of Text Embeddings')\n",
    "# ax.set_xlabel('Dimension 1')\n",
    "# ax.set_ylabel('Dimension 2')\n",
    "# ax.set_zlabel('Dimension 3')\n",
    "# plt.show()"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "S = {\n",
    "    'Class': [],\n",
    "    'Subclass': [],\n",
    "    'Caption': []\n",
    "}\n",
    "\n",
    "f = open('./desc4.txt', 'r')\n",
    "l = f.readlines()\n",
    "\n",
    "for line in l:\n",
    "    col = line.split('_')\n",
    "    c = col[0]\n",
    "    s = col[1]\n",
    "    cap = col[2]\n",
    "    S['Class'].append(f\"'{c}'\")\n",
    "    S['Subclass'].append(s)\n",
    "    S['Caption'].append(cap.rstrip('\\n'))\n",
    "\n",
    "df = pd.DataFrame(S)\n",
    "df.to_csv('./ShapeNetCore_Captions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A. Nichol, H. Jun, P. Dhariwal, P. Mishkin, and M. Chen, “Point-E: A System for Generating 3D Point Clouds from Complex Prompts,” arXiv:2212.08751 [cs], Dec. 2022, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2212.08751\n",
      "A. Radford et al., “Learning Transferable Visual Models From Natural Language Supervision,” arXiv:2103.00020 [cs], vol. 139, Feb. 2021, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2103.00020\n",
      "A. Ramesh et al., “Zero-Shot Text-to-Image Generation,” arXiv:2102.12092 [cs], Feb. 2021, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2102.12092\n",
      "A. X. Chang et al., “ShapeNet: An Information-Rich 3D Model Repository,” arXiv:1512.03012 [cs], Dec. 2015, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/1512.03012\n",
      "B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis,” arXiv:2003.08934 [cs], Aug. 2020, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2003.08934\n",
      "B. Poole, A. Jain, J. T. Barron, and B. Mildenhall, “DreamFusion: Text-to-3D using 2D Diffusion,” arXiv:2209.14988 [cs, stat], Sep. 2022, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2209.14988\n",
      "B. Zhou et al., “TinyLLaVA: A Framework of Small-scale Large Multimodal Models,” arXiv.org, 2024. https://arxiv.org/abs/2402.14289 (accessed Sep. 12, 2024).\n",
      "Blender Foundation, “blender.org - Home of the Blender project - Free and Open 3D Creation Software,” blender.org, 2019. https://www.blender.org/ (accessed Sep. 12, 2024).\n",
      "C.-H. Lin et al., “Magic3D: High-Resolution Text-to-3D Content Creation,” arXiv:2211.10440 [cs], Nov. 2022, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2211.10440\n",
      "E. R. Chan et al., “Efficient Geometry-aware 3D Generative Adversarial Networks,” arXiv.org, Apr. 27, 2022. https://arxiv.org/abs/2112.07945 (accessed Sep. 12, 2024).\n",
      "E. Sella, G. Fiebelman, P. Hedman, and H. Averbuch-Elor, “Vox-E: Text-guided Voxel Editing of 3D Objects,” arXiv.org, 2023. https://arxiv.org/abs/2303.12048 (accessed Sep. 12, 2024).\n",
      "F. Hong et al., “3DTopia: Large Text-to-3D Generation Model with Hybrid Diffusion Priors,” arXiv.org, 2024. https://arxiv.org/abs/2403.02234 (accessed Sep. 12, 2024).\n",
      "H. Chen et al., “Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction,” arXiv.org, 2023. https://arxiv.org/abs/2304.06714 (accessed Sep. 12, 2024).\n",
      "H. Jun and A. Nichol, “Shap-E: Generating Conditional 3D Implicit Functions,” arXiv.org, May 03, 2023. https://arxiv.org/abs/2305.02463 (accessed Sep. 12, 2024).\n",
      "H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual Instruction Tuning,” arXiv.org, Apr. 17, 2023. https://arxiv.org/abs/2304.08485 (accessed Sep. 12, 2024).\n",
      "H. Liu, C. Li, Y. Li, and Yong Jae Lee, “Improved Baselines with Visual Instruction Tuning,” arXiv (Cornell University), Oct. 2023, doi: https://doi.org/10.48550/arxiv.2310.03744.\n",
      "H. Xie, Z. Chen, F. Hong, and Z. Liu, “CityDreamer: Compositional Generative Model of Unbounded 3D Cities,” arXiv.org, 2023. https://arxiv.org/abs/2309.00610 (accessed Sep. 12, 2024).\n",
      "Hugging Face, “Hugging Face – On a mission to solve NLP, one commit at a time.,” huggingface.co, 2024. https://huggingface.co/ (accessed Sep. 12, 2024).\n",
      "J. Bai et al., “Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities,” arXiv.org, Aug. 24, 2023. https://arxiv.org/abs/2308.12966 (accessed Sep. 12, 2024).\n",
      "J. Gao et al., “GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images,” arXiv:2209.11163 [cs], Sep. 2022, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2209.11163\n",
      "J. Ho, A. Jain, and P. Abbeel, “Denoising Diffusion Probabilistic Models,” arXiv:2006.11239 [cs, stat], Dec. 2020, Available: https://arxiv.org/abs/2006.11239\n",
      "J. Li et al., “Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model,” arXiv.org, 2023. https://arxiv.org/abs/2311.06214 (accessed Sep. 12, 2024).\n",
      "J. Li, D. Li, S. Savarese, and S. Hoi, “BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,” arXiv:2301.12597 [cs], vol. 202, Jan. 2023, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2301.12597\n",
      "J. Yang, Z. Cheng, Y. Duan, P. Ji, and H. Li, “ConsistNet: Enforcing 3D Consistency for Multi-view Images Diffusion,” arXiv.org, 2023. https://arxiv.org/abs/2310.10343 (accessed Sep. 12, 2024).\n",
      "K. Chen, C. B. Choy, M. Savva, A. X. Chang, T. Funkhouser, and S. Savarese, “Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings,” arXiv.org, 2018. https://arxiv.org/abs/1803.08495 (accessed Sep. 12, 2024).\n",
      "L. Weng, “What are Diffusion Models?,” lilianweng.github.io, Jul. 11, 2021. https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ (accessed Sep. 12, 2024).\n",
      "L. Xue et al., “ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding,” arXiv.org, 2023. https://arxiv.org/abs/2305.08275 (accessed Sep. 12, 2024).\n",
      "M. Deitke et al., “Objaverse: A Universe of Annotated 3D Objects,” arXiv.org, Dec. 15, 2022. https://arxiv.org/abs/2212.08051 (accessed Sep. 12, 2024).\n",
      "M. Liu et al., “One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion,” arXiv.org, 2023. https://arxiv.org/abs/2311.07885 (accessed Sep. 12, 2024).\n",
      "M. Liu et al., “One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization,” arXiv.org, Jun. 29, 2023. https://arxiv.org/abs/2306.16928 (accessed Sep. 12, 2024).\n",
      "N. Müller, Y. Siddiqui, L. Porzi, S. R. Bulò, P. Kontschieder, and M. Nießner, “DiffRF: Rendering-Guided 3D Radiance Field Diffusion,” arXiv.org, 2022. https://arxiv.org/abs/2212.01206 (accessed Sep. 12, 2024).\n",
      "O. Avrahami, D. Lischinski, and O. Fried, “Blended Diffusion for Text-Driven Editing of Natural Images,” openaccess.thecvf.com, 2022.  https://openaccess.thecvf.com/content/CVPR2022/html/Avrahami_Blended_Diffusion_for_Text-Driven_Editing_of_Natural_Images_CVPR_2022_paper.html (accessed Sep. 12, 2024).\n",
      "R. Kar, “ShapeNetCore_Captions,” Huggingface.co, 2024. https://huggingface.co/datasets/Rohan3/ShapeNetCore_Captions (accessed Sep. 12, 2024).\n",
      "R. Liu, R. Wu, B. Van Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick, “Zero-1-to-3: Zero-shot One Image to 3D Object,” arXiv.org, Mar. 20, 2023. https://arxiv.org/abs/2303.11328 (accessed Sep. 12, 2024).\n",
      "R. Po et al., “State of the Art on Diffusion Models for Visual Computing,” arXiv.org, Oct. 11, 2023. https://arxiv.org/abs/2310.07204 (accessed Sep. 12, 2024).\n",
      "R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-Resolution Image Synthesis with Latent Diffusion Models,” arXiv:2112.10752 [cs], Apr. 2022, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2112.10752\n",
      "Rohankar7, “GitHub - rohankar7/Hope,” GitHub, 2024. https://github.com/rohankar7/Hope (accessed Sep. 12, 2024).\n",
      "S. J. Ryan, E. R. Chan, R. Po, Z. Ankner, J. Wu, and G. Wetzstein, “3D Neural Field Generation using Triplane Diffusion,” arXiv.org, 2022. https://arxiv.org/abs/2211.16677 (accessed Sep. 12, 2024).\n",
      "ShapeNet, “ShapeNetCore,” Huggingface.co, 2015. https://huggingface.co/datasets/ShapeNet/ShapeNetCore (accessed Sep. 12, 2024).\n",
      "T. Hu, F. Hong, and Z. Liu, “StructLDM: Structured Latent Diffusion for 3D Human Generation,” arXiv.org, 2024. https://arxiv.org/abs/2404.01241 (accessed Sep. 12, 2024).\n",
      "T. Luo, C. Rockwell, H. Lee, and J. Johnson, “Scalable 3D Captioning with Pretrained Models,” arXiv.org, 2023. https://arxiv.org/abs/2306.07279 (accessed Sep. 12, 2024).\n",
      "T. Wang et al., “Rodin: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion,” arXiv (Cornell University), Dec. 2022, doi: https://doi.org/10.48550/arxiv.2212.06135.\n",
      "X. Long et al., “Wonder3D: Single Image to 3D using Cross-Domain Diffusion,” arXiv.org, 2023. https://arxiv.org/abs/2310.15008 (accessed Sep. 12, 2024).\n",
      "Y. Hong et al., “LRM: Large Reconstruction Model for Single Image to 3D,” arXiv.org, Mar. 09, 2024. https://arxiv.org/abs/2311.04400 (accessed Sep. 12, 2024).\n",
      "Y. Shi, P. Wang, J. Ye, M. Long, K. Li, and X. Yang, “MVDream: Multi-view Diffusion for 3D Generation,” arXiv.org, Oct. 02, 2023. https://arxiv.org/abs/2308.16512 (accessed Sep. 12, 2024).\n",
      "Y. Xu et al., “DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model,” arXiv.org, 2023. https://arxiv.org/abs/2311.09217 (accessed Sep. 12, 2024).\n",
      "Z. Cao, F. Hong, T. Wu, L. Pan, and Z. Liu, “Large-Vocabulary 3D Diffusion Model with Transformer,” arXiv.org, 2023. https://arxiv.org/abs/2309.07920 (accessed Sep. 12, 2024).\n",
      "Z. Wu et al., “BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation,” arXiv.org, 2024. https://arxiv.org/abs/2401.17053 (accessed Sep. 12, 2024).\n"
     ]
    }
   ],
   "source": [
    "refs = [\n",
    "'A. X. Chang et al., “ShapeNet: An Information-Rich 3D Model Repository,” arXiv:1512.03012 [cs], Dec. 2015, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/1512.03012',\n",
    "'Hugging Face, “Hugging Face – On a mission to solve NLP, one commit at a time.,” huggingface.co, 2024. https://huggingface.co/ (accessed Sep. 12, 2024).',\n",
    "'E. R. Chan et al., “Efficient Geometry-aware 3D Generative Adversarial Networks,” arXiv.org, Apr. 27, 2022. https://arxiv.org/abs/2112.07945 (accessed Sep. 12, 2024).',\n",
    "'J. Gao et al., “GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images,” arXiv:2209.11163 [cs], Sep. 2022, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2209.11163',\n",
    "'A. Nichol, H. Jun, P. Dhariwal, P. Mishkin, and M. Chen, “Point-E: A System for Generating 3D Point Clouds from Complex Prompts,” arXiv:2212.08751 [cs], Dec. 2022, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2212.08751',\n",
    "'H. Jun and A. Nichol, “Shap-E: Generating Conditional 3D Implicit Functions,” arXiv.org, May 03, 2023. https://arxiv.org/abs/2305.02463 (accessed Sep. 12, 2024).',\n",
    "'S. J. Ryan, E. R. Chan, R. Po, Z. Ankner, J. Wu, and G. Wetzstein, “3D Neural Field Generation using Triplane Diffusion,” arXiv.org, 2022. https://arxiv.org/abs/2211.16677 (accessed Sep. 12, 2024).',\n",
    "'N. Müller, Y. Siddiqui, L. Porzi, S. R. Bulò, P. Kontschieder, and M. Nießner, “DiffRF: Rendering-Guided 3D Radiance Field Diffusion,” arXiv.org, 2022. https://arxiv.org/abs/2212.01206 (accessed Sep. 12, 2024).',\n",
    "'E. Sella, G. Fiebelman, P. Hedman, and H. Averbuch-Elor, “Vox-E: Text-guided Voxel Editing of 3D Objects,” arXiv.org, 2023. https://arxiv.org/abs/2303.12048 (accessed Sep. 12, 2024).',\n",
    "'Z. Cao, F. Hong, T. Wu, L. Pan, and Z. Liu, “Large-Vocabulary 3D Diffusion Model with Transformer,” arXiv.org, 2023. https://arxiv.org/abs/2309.07920 (accessed Sep. 12, 2024).',\n",
    "'B. Poole, A. Jain, J. T. Barron, and B. Mildenhall, “DreamFusion: Text-to-3D using 2D Diffusion,” arXiv:2209.14988 [cs, stat], Sep. 2022, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2209.14988',\n",
    "'C.-H. Lin et al., “Magic3D: High-Resolution Text-to-3D Content Creation,” arXiv:2211.10440 [cs], Nov. 2022, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2211.10440',\n",
    "'T. Wang et al., “Rodin: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion,” arXiv (Cornell University), Dec. 2022, doi: https://doi.org/10.48550/arxiv.2212.06135.',\n",
    "'H. Xie, Z. Chen, F. Hong, and Z. Liu, “CityDreamer: Compositional Generative Model of Unbounded 3D Cities,” arXiv.org, 2023. https://arxiv.org/abs/2309.00610 (accessed Sep. 12, 2024).',\n",
    "'T. Hu, F. Hong, and Z. Liu, “StructLDM: Structured Latent Diffusion for 3D Human Generation,” arXiv.org, 2024. https://arxiv.org/abs/2404.01241 (accessed Sep. 12, 2024).',\n",
    "'B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis,” arXiv:2003.08934 [cs], Aug. 2020, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2003.08934',\n",
    "'J. Ho, A. Jain, and P. Abbeel, “Denoising Diffusion Probabilistic Models,” arXiv:2006.11239 [cs, stat], Dec. 2020, Available: https://arxiv.org/abs/2006.11239',\n",
    "'R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-Resolution Image Synthesis with Latent Diffusion Models,” arXiv:2112.10752 [cs], Apr. 2022, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2112.10752',\n",
    "'Y. Shi, P. Wang, J. Ye, M. Long, K. Li, and X. Yang, “MVDream: Multi-view Diffusion for 3D Generation,” arXiv.org, Oct. 02, 2023. https://arxiv.org/abs/2308.16512 (accessed Sep. 12, 2024).',\n",
    "'K. Chen, C. B. Choy, M. Savva, A. X. Chang, T. Funkhouser, and S. Savarese, “Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings,” arXiv.org, 2018. https://arxiv.org/abs/1803.08495 (accessed Sep. 12, 2024).',\n",
    "'J. Li, D. Li, S. Savarese, and S. Hoi, “BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,” arXiv:2301.12597 [cs], vol. 202, Jan. 2023, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2301.12597',\n",
    "'T. Luo, C. Rockwell, H. Lee, and J. Johnson, “Scalable 3D Captioning with Pretrained Models,” arXiv.org, 2023. https://arxiv.org/abs/2306.07279 (accessed Sep. 12, 2024).',\n",
    "'A. Radford et al., “Learning Transferable Visual Models From Natural Language Supervision,” arXiv:2103.00020 [cs], vol. 139, Feb. 2021, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2103.00020',\n",
    "'L. Xue et al., “ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding,” arXiv.org, 2023. https://arxiv.org/abs/2305.08275 (accessed Sep. 12, 2024).',\n",
    "'H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual Instruction Tuning,” arXiv.org, Apr. 17, 2023. https://arxiv.org/abs/2304.08485 (accessed Sep. 12, 2024).',\n",
    "'B. Zhou et al., “TinyLLaVA: A Framework of Small-scale Large Multimodal Models,” arXiv.org, 2024. https://arxiv.org/abs/2402.14289 (accessed Sep. 12, 2024).',\n",
    "'H. Liu, C. Li, Y. Li, and Yong Jae Lee, “Improved Baselines with Visual Instruction Tuning,” arXiv (Cornell University), Oct. 2023, doi: https://doi.org/10.48550/arxiv.2310.03744.',\n",
    "'J. Bai et al., “Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities,” arXiv.org, Aug. 24, 2023. https://arxiv.org/abs/2308.12966 (accessed Sep. 12, 2024).',\n",
    "'R. Liu, R. Wu, B. Van Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick, “Zero-1-to-3: Zero-shot One Image to 3D Object,” arXiv.org, Mar. 20, 2023. https://arxiv.org/abs/2303.11328 (accessed Sep. 12, 2024).',\n",
    "'M. Liu et al., “One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization,” arXiv.org, Jun. 29, 2023. https://arxiv.org/abs/2306.16928 (accessed Sep. 12, 2024).',\n",
    "'M. Liu et al., “One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion,” arXiv.org, 2023. https://arxiv.org/abs/2311.07885 (accessed Sep. 12, 2024).',\n",
    "'H. Chen et al., “Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction,” arXiv.org, 2023. https://arxiv.org/abs/2304.06714 (accessed Sep. 12, 2024).',\n",
    "'Y. Xu et al., “DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model,” arXiv.org, 2023. https://arxiv.org/abs/2311.09217 (accessed Sep. 12, 2024).',\n",
    "'J. Li et al., “Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model,” arXiv.org, 2023. https://arxiv.org/abs/2311.06214 (accessed Sep. 12, 2024).',\n",
    "'Y. Hong et al., “LRM: Large Reconstruction Model for Single Image to 3D,” arXiv.org, Mar. 09, 2024. https://arxiv.org/abs/2311.04400 (accessed Sep. 12, 2024).',\n",
    "'X. Long et al., “Wonder3D: Single Image to 3D using Cross-Domain Diffusion,” arXiv.org, 2023. https://arxiv.org/abs/2310.15008 (accessed Sep. 12, 2024).',\n",
    "'J. Yang, Z. Cheng, Y. Duan, P. Ji, and H. Li, “ConsistNet: Enforcing 3D Consistency for Multi-view Images Diffusion,” arXiv.org, 2023. https://arxiv.org/abs/2310.10343 (accessed Sep. 12, 2024).',\n",
    "'R. Po et al., “State of the Art on Diffusion Models for Visual Computing,” arXiv.org, Oct. 11, 2023. https://arxiv.org/abs/2310.07204 (accessed Sep. 12, 2024).',\n",
    "'A. Ramesh et al., “Zero-Shot Text-to-Image Generation,” arXiv:2102.12092 [cs], Feb. 2021, Accessed: Sep. 12, 2024. [Online]. Available: https://arxiv.org/abs/2102.12092',\n",
    "'O. Avrahami, D. Lischinski, and O. Fried, “Blended Diffusion for Text-Driven Editing of Natural Images,” openaccess.thecvf.com, 2022.  https://openaccess.thecvf.com/content/CVPR2022/html/Avrahami_Blended_Diffusion_for_Text-Driven_Editing_of_Natural_Images_CVPR_2022_paper.html (accessed Sep. 12, 2024).',\n",
    "'Z. Wu et al., “BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation,” arXiv.org, 2024. https://arxiv.org/abs/2401.17053 (accessed Sep. 12, 2024).',\n",
    "'Blender Foundation, “blender.org - Home of the Blender project - Free and Open 3D Creation Software,” blender.org, 2019. https://www.blender.org/ (accessed Sep. 12, 2024).',\n",
    "'ShapeNet, “ShapeNetCore,” Huggingface.co, 2015. https://huggingface.co/datasets/ShapeNet/ShapeNetCore (accessed Sep. 12, 2024).',\n",
    "'L. Weng, “What are Diffusion Models?,” lilianweng.github.io, Jul. 11, 2021. https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ (accessed Sep. 12, 2024).',\n",
    "'M. Deitke et al., “Objaverse: A Universe of Annotated 3D Objects,” arXiv.org, Dec. 15, 2022. https://arxiv.org/abs/2212.08051 (accessed Sep. 12, 2024).',\n",
    "'R. Kar, “ShapeNetCore_Captions,” Huggingface.co, 2024. https://huggingface.co/datasets/Rohan3/ShapeNetCore_Captions (accessed Sep. 12, 2024).',\n",
    "'Rohankar7, “GitHub - rohankar7/Hope,” GitHub, 2024. https://github.com/rohankar7/Hope (accessed Sep. 12, 2024).',\n",
    "'F. Hong et al., “3DTopia: Large Text-to-3D Generation Model with Hybrid Diffusion Priors,” arXiv.org, 2024. https://arxiv.org/abs/2403.02234 (accessed Sep. 12, 2024).'\n",
    "]\n",
    "for i in sorted(refs):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
>>>>>>> 2a77acd35b6dbaf3bf2902a8daeb03793ad23b60
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "three",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
